{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through the crash estimation steps for one project and see where the numbers start to get off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, let's pick a project with a large/unreasonable number of crashes. Like >100 or so.\n",
    "crashes = pd.read_csv('output_2023_09_05/reports/safety-4-combined-b-crashes-all.csv')\n",
    "(crashes[crashes[\"ECmoj model\"] > 100])[\"Project ID\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project ID (picked the first one from table above): 64d2a1c2597e1e819a7b4309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the 'debug' folder for this project and pull out data to calculate the model crashes\n",
    "Actually if I can get as much stuff as possible from overall reports, that might make this easier (to be able to iterate/generalize to all projects later)\n",
    "\n",
    "Data needed:\n",
    "- Length, functional class, volume class for all segments/intersections in the project\n",
    "    - overall-5-ways.csv, overall-6-intersections.csv\n",
    "- Project length/count totals\n",
    "    - overall-2-reach-type.csv (or just calculate from overall-5-ways.csv and overall-6-intersections.csv)\n",
    "- Ljvf totals\n",
    "    - overall-3-reach-Ljvf.csv\n",
    "- Alpha constants\n",
    "    - safety-4-combined-a-crashes-model.csv\n",
    "- Volume/demand for all segments/intersections in the project\n",
    "    - overall-5-ways.csv, overall-6-intersections.csv\n",
    "- Volume/demand totals\n",
    "    - safety-5-volume-d-combined.csv\n",
    "- ECCmojvf (to compare against my manual results)\n",
    "    - safety-4-combined-a-crashes-model.csv\n",
    "- ECmoj (to compare against my manual results)\n",
    "    - safety-4-combined-b-crashes-all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = pd.read_csv('output_2023_09_05/reports/overall-5-ways.csv')\n",
    "intersections = pd.read_csv('output_2023_09_05/reports/overall-6-intersections.csv')\n",
    "alpha = pd.read_csv('output_2023_09_05/lookups/alpha.csv')\n",
    "Ljvf = pd.read_csv('output_2023_09_05/reports/overall-3-reach-Ljvf.csv')\n",
    "volume = pd.read_csv('output_2023_09_05/reports/safety-5-volume-d-combined.csv')\n",
    "crash_model = pd.read_csv('output_2023_09_05/reports/safety-4-combined-a-crashes-model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[segments[\"Project ID\"] == \"64d2a1c2597e1e819a7b4309\"]\n",
    "## Oops, this isn't in this table... I wonder why? anyway, let's choose a different project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crashes[\"Project ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(segments[\"Project ID\"].unique())\n",
    "## I wonder why this has 10 less projects than the other table...anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New project ID: 64962a7f1930d10600997fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all segments and intersections in project\n",
    "project_segments = segments[segments[\"Project ID\"] == \"64962a7f1930d10600997fdf\"]\n",
    "project_intersections = intersections[intersections[\"Project ID\"] == \"64962a7f1930d10600997fdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Ljvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find Ljvf - segments\n",
    "## Wait, what is the 'V Volume class' in Ljvf? Is it bicycle volume class or pedestrian volume class?\n",
    "## Based on technical documentation and emails from Matt, I think this should be bicycle volume class for roadways and pedestrian volume class for intersections\n",
    "## Group by type, volume class, functional class and sum length\n",
    "L_segment_vf = project_segments.groupby([\"Type\",\"Bicycle volume class\",\"Functional class\"])[\"Length\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also find total Ljvf - sum length (only keep separated by type)\n",
    "project_segments.groupby([\"Type\"])[\"Length\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find Ljvf - intersections\n",
    "L_intersection_vf = project_intersections.groupby([\"Type\",\"Pedestrian volume class\",\"Functional class\"])[\"Node ID\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also find total Ljvf - count all (only keep separated by type)\n",
    "project_intersections.groupby([\"Type\"])[\"Node ID\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Alpha constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find e^alpha from alpha constant\n",
    "alpha[\"e_alpha\"] = np.exp(alpha[\"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$j=roadway, EV_{mj}=\\sum_{w}E_{wm}$\n",
    "\n",
    "$j=intersection, EV_{mj}=\\sum_{i}E_{im}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"Not applicable\" with \"NaN\" to make it possible to be a float type\n",
    "project_segments_n = project_segments.replace(\"Not applicable\",np.NaN)\n",
    "project_intersections_n = project_intersections.replace(\"Not applicable\",np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_n[\"Bicycle exposure\"]=pd.to_numeric(project_segments_n[\"Bicycle exposure\"])\n",
    "project_segments_n[\"Pedestrian exposure\"]=pd.to_numeric(project_segments_n[\"Pedestrian exposure\"])\n",
    "project_intersections_n[\"Bicycle exposure\"]=pd.to_numeric(project_intersections_n[\"Bicycle exposure\"])\n",
    "project_intersections_n[\"Pedestrian exposure\"]=pd.to_numeric(project_intersections_n[\"Pedestrian exposure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Volume = sum of exposure across all ways/intersections\n",
    "V_bicycle_segment = project_segments_n.groupby([\"Type\"])[\"Bicycle exposure\"].sum()\n",
    "V_pedestrian_segment = project_segments_n.groupby([\"Type\"])[\"Pedestrian exposure\"].sum()\n",
    "V_bicycle_intersection = project_intersections_n.groupby([\"Type\"])[\"Bicycle exposure\"].sum()\n",
    "V_pedestrian_intersection = project_intersections_n.groupby([\"Type\"])[\"Pedestrian exposure\"].sum()\n",
    "print(V_bicycle_segment, V_pedestrian_segment, V_bicycle_intersection,V_pedestrian_intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1d. Crashes by functional/volume class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ECC_{cmojvf} = e^{â±­_{mojvf}} * L_{jvf} * (EV_{cmj})^{p}$\n",
    "\n",
    "$EC_{cmoj} = \\sum_{f}\\sum_{v}ECC_{cmojvf}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Starting with segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L_segment_vf.index = L_segment_vf.index.rename([\"Type\",\"volume\",\"functional class\"])\n",
    "L_segment_vf.index = L_segment_vf.index.set_levels(L_segment_vf.index.levels[1].str.lower(),level=1)\n",
    "L_segment_vf.loc[('network')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_segment_vf = pd.merge(alpha[alpha[\"location type\"] == \"roadway\"],L_segment_vf,on=[\"volume\",\"functional class\"],how='outer')\n",
    "alpha_L_segment_vf[\"e_alpha_Length\"] = alpha_L_segment_vf[\"e_alpha\"]*alpha_L_segment_vf[\"Length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vm_segment = pd.DataFrame(data={\"mode\":[\"bicycling\",\"walking\",\"combined\"],\"Vmj\":[V_bicycle_segment[0],V_pedestrian_segment[0],V_bicycle_segment[0] + V_pedestrian_segment[0]]})\n",
    "alpha_L_segment_vf_V_m = pd.merge(alpha_L_segment_vf, Vm_segment,on=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_segment_vf_V_m[\"Vmj_p\"] = pow(alpha_L_segment_vf_V_m[\"Vmj\"],0.5)\n",
    "alpha_L_segment_vf_V_m[\"e_alpha_Vmj_p\"] = alpha_L_segment_vf_V_m[\"e_alpha\"]*alpha_L_segment_vf_V_m[\"Vmj_p\"]\n",
    "alpha_L_segment_vf_V_m[\"ECCmojvf\"] = alpha_L_segment_vf_V_m[\"e_alpha_Length\"]*alpha_L_segment_vf_V_m[\"Vmj_p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This is e^alpha * Volume which should be crashes/mile\n",
    "alpha_L_segment_vf_V_m.groupby([\"mode\",\"outcome\"])[\"e_alpha_Vmj_p\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Crashes\n",
    "alpha_L_segment_vf_V_m.groupby([\"mode\",\"outcome\"])[\"ECCmojvf\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ECmoj (summed across volume and functional classes)\n",
    "alpha_L_segment_vf_V_m.groupby([\"mode\",\"outcome\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when calculated manually, segments also have hundreds/thousands of crashes!? Clearly it seems like there is some problem or difference in the way the tool is calculating crashes from the given data (regardless of whether there are also issues with the underlying data/constants). This might have something to do with the Ljvf calculation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_intersection_vf.index = L_intersection_vf.index.rename([\"Type\",\"volume\",\"functional class\"])\n",
    "L_intersection_vf.index = L_intersection_vf.index.set_levels(L_intersection_vf.index.levels[1].str.lower(),level=1)\n",
    "L_intersection_vf.loc[('network')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[alpha[\"location type\"] == \"intersection\"]\n",
    "alpha_L_intersection_vf = pd.merge(alpha[alpha[\"location type\"] == \"intersection\"],L_intersection_vf,on=[\"volume\",\"functional class\"],how='outer')\n",
    "alpha_L_intersection_vf[\"e_alpha_Count\"] = alpha_L_intersection_vf[\"e_alpha\"]*alpha_L_intersection_vf[\"Node ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vm_intersection = pd.DataFrame(data={\"mode\":[\"bicycling\",\"walking\",\"combined\"],\"Vmj\":[V_bicycle_intersection[0],V_pedestrian_intersection[0],V_bicycle_intersection[0] + V_pedestrian_intersection[0]]})\n",
    "alpha_L_intersection_vf_V_m = pd.merge(alpha_L_intersection_vf, Vm_intersection,on=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_intersection_vf_V_m[\"Vmj_p\"] = pow(alpha_L_intersection_vf_V_m[\"Vmj\"],0.5)\n",
    "alpha_L_intersection_vf_V_m[\"e_alpha_Vmj_p\"] = alpha_L_intersection_vf_V_m[\"e_alpha\"]*alpha_L_intersection_vf_V_m[\"Vmj_p\"]\n",
    "alpha_L_intersection_vf_V_m[\"ECCmojvf\"] = alpha_L_intersection_vf_V_m[\"e_alpha_Count\"]*alpha_L_intersection_vf_V_m[\"Vmj_p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is e^alpha * Volume which should be crashes/intersection (and then there are 45 intersections)\n",
    "alpha_L_intersection_vf_V_m.groupby([\"mode\",\"outcome\"])[\"e_alpha_Vmj_p\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crashes\n",
    "alpha_L_intersection_vf_V_m.groupby([\"mode\",\"outcome\"])[\"ECCmojvf\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ECmoj (summed across volume and functional classes)\n",
    "## alpha_L_intersection_vf_V_m.groupby([\"mode\",\"outcome\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_intersection_vf_V_m.groupby(\"mode\")[\"ECCmojvf\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like there are just a lot of crashes/intersection multiplied over a large number of intersections. 10-12 crashes per intersection per year actually even sounds kind of reasonable. That is only ~1 crash per month. The problem seems to be that this is just directly multiplied across all 45 intersections, while it seems like crashes in real life would not happen that consistently at every intersection in the project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare with the Ljvf, Vmj, ECCmojvf, ECmoj used in the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ljvf intersection - tool\n",
    "Ljvf[Ljvf[\"Project ID\"] == \"64962a7f1930d10600997fdf\"][Ljvf[\"J location type\"] == \"intersection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ljvf intersection - manual\n",
    "L_intersection_vf.loc['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ljvf segment - tool\n",
    "Ljvf[Ljvf[\"Project ID\"] == \"64962a7f1930d10600997fdf\"][Ljvf[\"J location type\"] == \"roadway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ljvf segment - manual\n",
    "L_segment_vf.loc['network']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So intersection Ljvf is calculated in the same way - but segment Ljvf is scaled to a fraction somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh - I wonder if this is being converted from feet to miles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_segment_vf.loc['network']/5280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! The tool is just converting from feet to miles\n",
    "\n",
    "Recalculate crashes with this Ljvf based on miles instead of feet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_segment_vf_miles = L_segment_vf.loc['network']/5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_segment_vf = pd.merge(alpha[alpha[\"location type\"] == \"roadway\"],L_segment_vf_miles,on=[\"volume\",\"functional class\"],how='outer')\n",
    "alpha_L_segment_vf[\"e_alpha_Length\"] = alpha_L_segment_vf[\"e_alpha\"]*alpha_L_segment_vf[\"Length\"]\n",
    "Vm_segment = pd.DataFrame(data={\"mode\":[\"bicycling\",\"walking\",\"combined\"],\"Vmj\":[V_bicycle_segment[0],V_pedestrian_segment[0],V_bicycle_segment[0] + V_pedestrian_segment[0]]})\n",
    "alpha_L_segment_vf_V_m = pd.merge(alpha_L_segment_vf, Vm_segment,on=\"mode\")\n",
    "alpha_L_segment_vf_V_m[\"Vmj_p\"] = pow(alpha_L_segment_vf_V_m[\"Vmj\"],0.5)\n",
    "alpha_L_segment_vf_V_m[\"e_alpha_Vmj_p\"] = alpha_L_segment_vf_V_m[\"e_alpha\"]*alpha_L_segment_vf_V_m[\"Vmj_p\"]\n",
    "alpha_L_segment_vf_V_m[\"ECCmojvf\"] = alpha_L_segment_vf_V_m[\"e_alpha_Length\"]*alpha_L_segment_vf_V_m[\"Vmj_p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crashes\n",
    "alpha_L_segment_vf_V_m.groupby([\"mode\",\"outcome\"])[\"ECCmojvf\"].plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_L_segment_vf_V_m.groupby([\"mode\",\"outcome\"]).sum()\n",
    "## So this looks a lot more reasonable and closer to what the tool has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try a new method - calculate crashes individually per segment/intersection (only add up at the very end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if it would actually be possible to model crashes at the segment/intersection level?\n",
    "1. Match alpha to the segments table - volume (exposure) and length are already included\n",
    "2. Multiply everything out\n",
    "3. Then I can see the actual crashes/segment or crashes/intersection, split up into the individual segments and intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First - segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns to match each other and combine tables\n",
    "project_segments_n[\"Bicycle volume class\"] = project_segments_n[\"Bicycle volume class\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_segment = alpha[alpha[\"location type\"] == \"roadway\"].rename(columns={\"volume\":\"Bicycle volume class\",\"functional class\":\"Functional class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_alpha = pd.merge(project_segments_n,alpha_segment, on=[\"Bicycle volume class\",\"Functional class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to miles - what I didn't notice earlier!\n",
    "project_segments_alpha[\"Length_miles\"] = project_segments_alpha[\"Length\"]/5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set volume separately by mode\n",
    "project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"bicycling\", \"Volume\"] = project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"bicycling\", \"Bicycle exposure\"]\n",
    "project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"walking\", \"Volume\"] = project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"walking\", \"Pedestrian exposure\"]\n",
    "project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"combined\", \"Volume\"] = (project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"combined\", \"Bicycle exposure\"] + project_segments_alpha.loc[project_segments_alpha[\"mode\"] == \"combined\", \"Pedestrian exposure\"])\n",
    "project_segments_alpha[\"Volume_p\"] = pow(project_segments_alpha[\"Volume\"],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_alpha[\"e_alpha_Length\"] = project_segments_alpha[\"e_alpha\"]*project_segments_alpha[\"Length_miles\"]\n",
    "project_segments_alpha[\"e_alpha_Volume_p\"] = project_segments_alpha[\"e_alpha\"]*project_segments_alpha[\"Volume_p\"]\n",
    "project_segments_alpha[\"Crashes\"] = project_segments_alpha[\"e_alpha_Length\"]*project_segments_alpha[\"Volume_p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_alpha.groupby([\"mode\",\"outcome\"])[\"Crashes\"].plot(legend=True,figsize=(15,10))\n",
    "## So this is the estimated crashes at each individual segment - which actually looks kind of reasonable. 0.1 - 0.2 crashes/year, totalled over all of the segments, \"sounds ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next - intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns to match each other and combine tables\n",
    "project_intersections_n[\"Pedestrian volume class\"] = project_intersections_n[\"Pedestrian volume class\"].str.lower()\n",
    "alpha_intersection = alpha[alpha[\"location type\"] == \"intersection\"].rename(columns={\"volume\":\"Pedestrian volume class\",\"functional class\":\"Functional class\"})\n",
    "project_intersections_alpha = pd.merge(project_intersections_n,alpha_intersection, on=[\"Pedestrian volume class\",\"Functional class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate volume separately by mode\n",
    "project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"bicycling\", \"Volume\"] = project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"bicycling\", \"Bicycle exposure\"]\n",
    "project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"walking\", \"Volume\"] = project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"walking\", \"Pedestrian exposure\"]\n",
    "project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"combined\", \"Volume\"] = (project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"combined\", \"Bicycle exposure\"] + project_intersections_alpha.loc[project_intersections_alpha[\"mode\"] == \"combined\", \"Pedestrian exposure\"])\n",
    "project_intersections_alpha[\"Volume_p\"] = pow(project_intersections_alpha[\"Volume\"],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project_intersections_alpha[\"e_alpha_Length\"] = project_segments_alpha[\"e_alpha\"]*project_segments_alpha[\"Length_miles\"]\n",
    "## No multiplying by length or count here - the e^alpha is just crashes/volume for that one intersection\n",
    "project_intersections_alpha[\"e_alpha_Volume_p\"] = project_intersections_alpha[\"e_alpha\"]*project_intersections_alpha[\"Volume_p\"]\n",
    "project_intersections_alpha[\"Crashes\"] = project_intersections_alpha[\"e_alpha_Volume_p\"]\n",
    "project_intersections_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project_intersections_alpha.groupby([\"mode\",\"outcome\"])[\"Crashes\"].plot(legend=True,figsize=(15,10))\n",
    "## So this is the estimated crashes at each individual intersection - which also actually looks kind of reasonable. 0.1-0.5 crashes/intersection/year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And this is comparing both. Clearly the main difference is that there are just more crashes per intersection than there are per segment\n",
    "# but I think that is expected behavior - since intersections are usually the most dangerous places with a majority of crashes\n",
    "# So maybe there actually isn't a problem - maybe this actually makes sense?\n",
    "# Let's look at total crashes next and see if that is where the numbers start to look so unreasonable\n",
    "project_segments_alpha.groupby([\"mode\",\"outcome\"])[\"Crashes\"].plot(legend=True,figsize=(15,10))\n",
    "project_intersections_alpha.groupby([\"mode\",\"outcome\"])[\"Crashes\"].plot(legend=True,figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, total crashes by mode/outcome/location type (ECmoj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_alpha.groupby([\"mode\",\"outcome\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_intersections_alpha.groupby([\"mode\",\"outcome\"])[\"Crashes\"].sum()\n",
    "## Wait! These numbers are way lower than what are in the tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare segment crashes and intersection crashes from the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool crashes - only look at mean since it's all the same - ECmoj doesn't have any different estimates\n",
    "project_crashes = crashes[crashes[\"Project ID\"] == \"64962a7f1930d10600997fdf\"][crashes[\"K estimate\"] == \"mean\"]\n",
    "project_crashes_intersections = project_crashes[project_crashes[\"J Location\"] == \"intersection\"]\n",
    "project_crashes_segments = project_crashes[project_crashes[\"J Location\"] == \"roadway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_crashes_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_segments_alpha.groupby([\"mode\",\"outcome\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_crashes_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_intersections_alpha.groupby([\"mode\",\"outcome\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So these individual calculations are much lower than when it is calculated at a more aggregate level directly!?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
